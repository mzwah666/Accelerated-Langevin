{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69d2037-37a7-4e29-a12b-769251e151ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa7aa34-493f-4c31-a3a8-55c6a0d62e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_beta(beta0, i, c=1):\n",
    "    beta = np.log(np.exp(c*beta0)+i)/c\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2676245a-db36-4019-bbff-8f148edaa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = Bounds([0], [np.inf])\n",
    "def langevin(x0, d, func, grad, maxiter = 100, eta0 = 0.001, beta0 = 1, beta_schedule = log_beta, c=1):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Langevin Algorithm with exact linesearch on the step size eta\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0 (float): initial value for eta\n",
    "        beta0 (float): initial value for beta\n",
    "        beta_schedule (Callable): annealing schedule for temperature, starting with beta0 and ending with beta1\n",
    "        c (float): constant in logarithmic annealing schedule\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    x_list[0,:] = x0\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    f_list[0] = func(x0)\n",
    "    for i in range(1,maxiter):\n",
    "        epsilon = np.random.normal(0, 1, d)\n",
    "        beta = beta_schedule(beta0, i, c=c)\n",
    "        def objective_function(eta):\n",
    "            return func(x_list[i-1,:]- eta*grad(x_list[i-1,:]) + np.sqrt(2*eta/beta)*epsilon)\n",
    "        # perform exact linesearch\n",
    "        result = minimize(objective_function, eta0, method = \"SLSQP\", bounds=bounds)\n",
    "        eta = result.x\n",
    "        x_list[i,:] = x_list[i-1,:] - eta*grad(x_list[i-1,:]) + np.sqrt(2*eta/beta)*epsilon\n",
    "        f_list[i] = func(x_list[i,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83479c70-4b3d-4291-9736-fe6350c7f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, d, func, grad, maxiter = 100, eta0 = 0.001):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Descent with exact linesearch on the step size eta\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0 (float): initial value for eta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    x_list[0,:] = x0\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    f_list[0] = func(x0)\n",
    "    for i in range(1,maxiter):\n",
    "        def objective_function(eta):\n",
    "            return func(x_list[i-1,:]- eta*grad(x_list[i-1,:]))\n",
    "        # perform exact linesearch\n",
    "        result = minimize(objective_function, eta0, method = \"SLSQP\", bounds=bounds)\n",
    "        eta = result.x\n",
    "        x_list[i,:] = x_list[i-1,:] - eta*grad(x_list[i-1,:])\n",
    "        f_list[i] = func(x_list[i,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "501aa26b-4ecb-4b50-8bd6-ec04bbddabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_agd_x_noise(x0, d, func, grad, theta, maxiter = 100, L = None, C1 = 1):\n",
    "    \"\"\"\n",
    "    This code implements Accelerated Langevin with Perturbed x\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        theta (float): momentum parameter\n",
    "        L (float): Lipschitz constant\n",
    "        C1 (float): constant for scaling beta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    y_list = np.zeros((maxiter,d))\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    x_list[0,:] = x0\n",
    "    f_list[0] = func(x_list[0,:])\n",
    "    for i in range(maxiter-1):\n",
    "        eps = np.random.normal(0, 1, size = (d,))\n",
    "        if L is not None:\n",
    "            eta = min(C0/np.sqrt(i+1), theta/(6*L))\n",
    "        else:\n",
    "            eta = C0/((i+1)**(2/3))\n",
    "        beta = C1*(i+1)**(2/3)\n",
    "        xi = x_list[i,:] + np.sqrt(2*eta/beta)*eps\n",
    "        y_list[i+1,:] = xi - eta*grad(xi)\n",
    "        x_list[i+1,:] = y_list[i+1,:] + (1-theta)*(y_list[i+1,:]-y_list[i,:])\n",
    "        f_list[i+1] = func(x_list[i+1,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f6a830-ef7a-4426-80fb-54c67fa66a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_agd_y_noise(x0, d, func, grad, theta, maxiter = 100, L = None, C1 = 1):\n",
    "    \"\"\"\n",
    "    This code implements Accelerated Langevin with Perturbed y\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        theta (float): momentum parameter\n",
    "        L (float): Lipschitz constant\n",
    "        C1 (float): constant for scaling beta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    y_list = np.zeros((maxiter,d))\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    x_list[0,:] = x0\n",
    "    f_list[0] = func(x_list[0,:])\n",
    "    for i in range(maxiter-1):\n",
    "        eps = np.random.normal(0, 1, size = (d,))\n",
    "        if L is not None:\n",
    "            eta = min(C0/np.sqrt(i+1), theta/(3*L))\n",
    "        else:\n",
    "            eta = C0/((i+1)**(2/3))\n",
    "        beta = C1*(i+1)**(2/3)\n",
    "        y_list[i+1,:] = x_list[i,:] - eta*grad(x_list[i,:]) + np.sqrt(2*eta/beta)*eps\n",
    "        x_list[i+1,:] = y_list[i+1,:] + (1-theta)*(y_list[i+1,:]-y_list[i,:])\n",
    "        f_list[i+1] = func(x_list[i+1,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43ae65ca-8462-4d2b-a558-3a396e7df0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(d, func, grad, maxiter=400, eta0_gld=1e-2, eta0_gd=1e-2, beta0=1, beta_schedule=log_beta, c=1, var=1, C0=1, C1=1, theta=0.9):\n",
    "    \"\"\"\n",
    "    This code compares Gradient Langevin, Gradient Descent and Accelerated Langevin by:\n",
    "    1) plotting the average convergence curve in 100 runs;\n",
    "    2) computing Q1, median and Q3 of the final value of both algorithms.\n",
    "\n",
    "    Args:\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0_gld (float): initial value for eta used in Gradient Langevin\n",
    "        eta0_gd (float): initial value for eta used in Gradient Descent\n",
    "        beta0 (float): initial value for beta\n",
    "        beta_schedule (Callable): annealing schedule for temperature, starting with beta0\n",
    "        c (float): parameter in annealing schedule\n",
    "        var (float): variance of the distribution where the initial point is sampled from normal distribution\n",
    "        C0 (float): constant for scaling eta in Accelerated Langevin\n",
    "        C1 (float): constant for scaling beta in Accelerated Langevin\n",
    "        theta (float): momentum parameter\n",
    "    \n",
    "    \"\"\"\n",
    "    gld_result = None\n",
    "    gd_result = None\n",
    "    perturb_x_result = None\n",
    "    perturb_y_result = None\n",
    "    x = np.zeros((100,d))\n",
    "    gld_x = np.zeros((100,d))\n",
    "    gd_x = np.zeros((100,d))\n",
    "    perturb_x = np.zeros((100,d))\n",
    "    perturb_y = np.zeros((100,d))\n",
    "    n_x = np.zeros((100,d))\n",
    "    for i in tqdm(range(100)):\n",
    "        x0 = np.random.normal(0,var, size=(d,))\n",
    "        x[i,:] = x0\n",
    "        f_list_gld, x_list_gld = langevin(x0, d, func, grad, maxiter=maxiter, eta0 = eta0_gld, beta0 = beta0, beta1=beta1, beta_schedule = beta_schedule, c=c)\n",
    "        f_list_grad, x_list_gd = gradient_descent(x0, d, func, grad, maxiter=maxiter, eta0 = eta0_gd)\n",
    "        f_list_perturb_x, x_list_perturb_x = langevin_agd_x_noise(x0, d, func, grad, theta = theta, maxiter=maxiter, C0 = C0, C1 = C1)\n",
    "        f_list_perturb_y, x_list_perturb_y = langevin_agd_y_noise(x0, d, func, grad, theta = theta, maxiter=maxiter, C0 = C0, C1 = C1)\n",
    "        \n",
    "        gld_x[i,:] = x_list_gld[-1,:]\n",
    "        gd_x[i,:] = x_list_gd[-1,:]\n",
    "        perturb_x[i,:] = x_list_perturb_x[-1,:]\n",
    "        perturb_y[i,:] = x_list_perturb_y[-1,:]\n",
    "        \n",
    "        f_list_gld = np.array(f_list_gld)\n",
    "        f_list_grad = np.array(f_list_grad)\n",
    "        f_list_perturb_x = np.array(f_list_perturb_x)\n",
    "        f_list_perturb_y = np.array(f_list_perturb_y)\n",
    "        f_list_gld = f_list_gld.reshape((maxiter,1))\n",
    "        f_list_grad = f_list_grad.reshape((maxiter,1))\n",
    "        f_list_perturb_x = f_list_perturb_x.reshape((maxiter,1))\n",
    "        f_list_perturb_y = f_list_perturb_y.reshape((maxiter,1))\n",
    "        \n",
    "        if i == 0:\n",
    "            gld_result = f_list_gld\n",
    "            gd_result = f_list_grad\n",
    "            perturb_x_result = f_list_perturb_x\n",
    "            perturb_y_result = f_list_perturb_y\n",
    "            \n",
    "        else:\n",
    "            gld_result = np.concatenate((gld_result, f_list_gld), axis = 1)\n",
    "            gd_result = np.concatenate((gd_result, f_list_grad), axis = 1)\n",
    "            perturb_x_result = np.concatenate((perturb_x_result, f_list_perturb_x), axis = 1)\n",
    "            perturb_y_result = np.concatenate((perturb_y_result, f_list_perturb_y), axis = 1)\n",
    "\n",
    "    # Extract final output\n",
    "    gld_final = gld_result[-1,:]\n",
    "    gd_final = gd_result[-1,:]\n",
    "    perturb_x_final = perturb_x_result[-1,:]\n",
    "    perturb_y_final = perturb_y_result[-1,:]\n",
    "\n",
    "    mean_gld = np.mean(gld_result, axis=1)\n",
    "    std_dev_gld = np.std(gld_result, axis=1)\n",
    "    mean_gd = np.mean(gd_result, axis=1)\n",
    "    std_dev_gd = np.std(gd_result, axis=1)\n",
    "    mean_perturb_x = np.mean(perturb_x_result, axis=1)\n",
    "    std_dev_perturb_x = np.std(perturb_x_result, axis=1)\n",
    "    mean_perturb_y = np.mean(perturb_y_result, axis=1)\n",
    "    std_dev_perturb_y = np.std(perturb_y_result, axis=1)\n",
    "\n",
    "    gld_mean = statistics.mean(gld_final)\n",
    "    q1, q2, q3 = statistics.quantiles(gld_final, n=4)  # q2 == median\n",
    "    print(\"Q1, Q2(median), Q3, mean for Gradient Langevin:\", q1, q2, q3, gld_mean)\n",
    "    gd_mean = statistics.mean(gd_final)\n",
    "    q1, q2, q3 = statistics.quantiles(gd_final, n=4)  # q2 == median\n",
    "    print(\"Q1, Q2(median), Q3, mean for Gradient Descent:\", q1, q2, q3, gd_mean)\n",
    "    perturb_x_mean = statistics.mean(perturb_x_final)\n",
    "    q1, q2, q3 = statistics.quantiles(perturb_x_final, n=4)  # q2 == median\n",
    "    print(\"Q1, Q2(median), Q3, mean for Perturb x:\", q1, q2, q3, perturb_x_mean)\n",
    "    perturb_y_mean = statistics.mean(perturb_y_final)\n",
    "    q1, q2, q3 = statistics.quantiles(perturb_y_final, n=4)  # q2 == median\n",
    "    print(\"Q1, Q2(median), Q3, mean for Perturb y:\", q1, q2, q3, perturb_y_mean)\n",
    "    n_mean = statistics.mean(n_final)\n",
    "\n",
    "    if d == 2:\n",
    "        x_s = np.linspace(-4*var, 4*var, 400)\n",
    "        y_s = np.linspace(-4*var, 4*var, 400)\n",
    "        X, Y = np.meshgrid(x_s, y_s)\n",
    "        XY = np.stack([X.ravel(), Y.ravel()], axis=1) \n",
    "        Z = np.array([func(xy) for xy in XY])\n",
    "        Z = Z.reshape(X.shape)\n",
    "        \n",
    "        \n",
    "        plt.plot(np.arange(0,maxiter), np.log(mean_gld), label = \"Gradient Langevin\")\n",
    "        plt.plot(np.arange(0,maxiter), np.log(mean_gd), label = \"Gradient Descent\")\n",
    "        plt.plot(np.arange(0,maxiter), np.log(mean_perturb_x), label = \"Perturb x\")\n",
    "        plt.plot(np.arange(0,maxiter), np.log(mean_perturb_y), label = \"Perturb y\")\n",
    "        # plt.plot(np.arange(0,maxiter), np.log(mean_n+5.162), label = \"Nesterov\")\n",
    "        # plt.ylim(0,5)\n",
    "        plt.xlabel('Number of Gradient Evaluations')\n",
    "        plt.ylabel('log(f(x_k)-f*)')\n",
    "        plt.legend()\n",
    "        plt.savefig('benchmark.jpg')\n",
    "        \n",
    "    else:\n",
    "        plt.plot(np.arange(0,maxiter), np.log10(mean_gld), label = \"Gradient Langevin\")\n",
    "        plt.plot(np.arange(0,maxiter), np.log10(mean_gd), label = \"Gradient Descent\")\n",
    "        plt.plot(np.arange(0,maxiter), np.log10(mean_perturb_x), label = \"Perturb x\")\n",
    "        plt.plot(np.arange(0,maxiter), np.log10(mean_perturb_y), label = \"Perturb y\")\n",
    "        plt.legend()\n",
    "        plt.xlim(-4*var, 4*var)\n",
    "        plt.ylim(-4*var, 4*var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a5fa1b1-0401-4087-bea7-1a0490081a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multistart(d, \n",
    "            func, \n",
    "            grad, \n",
    "            maxiter=400, \n",
    "            eta0_gld=1e-2, \n",
    "            eta0_gd=1e-2, \n",
    "            beta0=1, \n",
    "            beta_schedule=log_beta, \n",
    "            c=1, \n",
    "            var=1, \n",
    "            C0=1, \n",
    "            C1=1, \n",
    "            theta=0.9,\n",
    "            x_min=0,\n",
    "            x_max=3,\n",
    "            y_min=0,\n",
    "            y_max=3):\n",
    "    \"\"\"\n",
    "    This code compares Gradient Langevin, Gradient Descent and Accelerated Langevin by combining them with multistart method\n",
    "\n",
    "    Args:\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0_gld (float): initial value for eta used in Gradient Langevin\n",
    "        eta0_gd (float): initial value for eta used in Gradient Descent\n",
    "        beta0 (float): initial value for beta\n",
    "        beta_schedule (Callable): annealing schedule for temperature, starting with beta0\n",
    "        c (float): parameter in annealing schedule\n",
    "        var (float): variance of the distribution where the initial point is sampled from normal distribution\n",
    "        C0 (float): constant for scaling eta in Accelerated Langevin\n",
    "        C1 (float): constant for scaling beta in Accelerated Langevin\n",
    "        theta (float): momentum parameter\n",
    "        x_min (float): min of x\n",
    "        x_max (float): max of x\n",
    "        y_min (float): min of y\n",
    "        y_max (float): max of y\n",
    "    \n",
    "    \"\"\"\n",
    "    gld_result = None\n",
    "    gd_result = None\n",
    "    perturb_x_result = None\n",
    "    perturb_y_result = None\n",
    "    x = np.zeros((100,d))\n",
    "    gld_x = np.zeros((100,d))\n",
    "    gd_x = np.zeros((100,d))\n",
    "    perturb_x = np.zeros((100,d))\n",
    "    perturb_y = np.zeros((100,d))\n",
    "    n_x = np.zeros((100,d))\n",
    "    # Number of points you want\n",
    "    n_points = 100  \n",
    "    \n",
    "    \n",
    "    # Uniform sampling in each dimension\n",
    "    x_coords = np.random.uniform(x_min, x_max, n_points)\n",
    "    y_coords = np.random.uniform(y_min, y_max, n_points)\n",
    "    # Combine into (n_points, 2) array\n",
    "    points = np.column_stack((x_coords, y_coords))\n",
    "\n",
    "    for i in tqdm(range(100)):\n",
    "        x0 = points[i,:]\n",
    "        x[i,:] = x0\n",
    "        f_list_gld, x_list_gld = langevin(x0, d, func, grad, maxiter=maxiter, eta0 = eta0_gld, beta0 = beta0, beta1=beta1, beta_schedule = beta_schedule, c=c)\n",
    "        f_list_grad, x_list_gd = gradient_descent(x0, d, func, grad, maxiter=maxiter, eta0 = eta0_gd)\n",
    "        f_list_perturb_x, x_list_perturb_x = langevin_agd_x_noise(x0, d, func, grad, theta = theta, maxiter=maxiter, C0 = C0, C1 = C1)\n",
    "        f_list_perturb_y, x_list_perturb_y = langevin_agd_y_noise(x0, d, func, grad, theta = theta, maxiter=maxiter, C0 = C0, C1 = C1)\n",
    "        \n",
    "        gld_x[i,:] = x_list_gld[-1,:]\n",
    "        gd_x[i,:] = x_list_gd[-1,:]\n",
    "        perturb_x[i,:] = x_list_perturb_x[-1,:]\n",
    "        perturb_y[i,:] = x_list_perturb_y[-1,:]\n",
    "        f_list_gld = np.array(f_list_gld)\n",
    "        f_list_grad = np.array(f_list_grad)\n",
    "        f_list_perturb_x = np.array(f_list_perturb_x)\n",
    "        f_list_perturb_y = np.array(f_list_perturb_y)\n",
    "        \n",
    "        f_list_gld = f_list_gld.reshape((maxiter,1))\n",
    "        f_list_grad = f_list_grad.reshape((maxiter,1))\n",
    "        f_list_perturb_x = f_list_perturb_x.reshape((maxiter,1))\n",
    "        f_list_perturb_y = f_list_perturb_y.reshape((maxiter,1))\n",
    "        \n",
    "        if i == 0:\n",
    "            gld_result = f_list_gld\n",
    "            gd_result = f_list_grad\n",
    "            perturb_x_result = f_list_perturb_x\n",
    "            perturb_y_result = f_list_perturb_y\n",
    "\n",
    "        else:\n",
    "            gld_result = np.concatenate((gld_result, f_list_gld), axis = 1)\n",
    "            gd_result = np.concatenate((gd_result, f_list_grad), axis = 1)\n",
    "            perturb_x_result = np.concatenate((perturb_x_result, f_list_perturb_x), axis = 1)\n",
    "            perturb_y_result = np.concatenate((perturb_y_result, f_list_perturb_y), axis = 1)\n",
    "\n",
    "    # Extract final output\n",
    "    gld_final = gld_result[-1,:]\n",
    "    gd_final = gd_result[-1,:]\n",
    "    perturb_x_final = perturb_x_result[-1,:]\n",
    "    perturb_y_final = perturb_y_result[-1,:]\n",
    "\n",
    "    min_gld = np.min(gld_final)\n",
    "    min_gd = np.min(gd_final)\n",
    "    min_perturb_x = np.min(perturb_x_final)\n",
    "    min_perturb_y = np.min(perturb_y_final)\n",
    "    \n",
    "\n",
    "    print(\"Min for GLD\", min_gld)\n",
    "    print(\"Min for GD\", min_gd)\n",
    "    print(\"Min for perturb x\", min_perturb_x)\n",
    "    print(\"Min for perturb y\", min_perturb_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280f052-a1de-4151-a2bc-adc88aaea65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
