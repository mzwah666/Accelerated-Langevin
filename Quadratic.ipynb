{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787b1381-dc39-4078-955a-c2eb4561c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a6ab06-cded-4003-a8ec-1779cbede5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_beta(beta0, i, c=1):\n",
    "    beta = np.log(np.exp(c*beta0)+i)/c\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd31eb0-8110-478f-93f4-381b31978c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = Bounds([0], [np.inf])\n",
    "def langevin(x0, d, func, grad, maxiter = 100, eta0 = 0.001, beta0 = 1, beta_schedule = log_beta, c=1):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Langevin Algorithm with exact linesearch on the step size eta\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0 (float): initial value for eta\n",
    "        beta0 (float): initial value for beta\n",
    "        beta_schedule (Callable): annealing schedule for temperature, starting with beta0 and ending with beta1\n",
    "        c (float): constant in logarithmic annealing schedule\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    x_list[0,:] = x0\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    f_list[0] = func(x0)\n",
    "    for i in range(1,maxiter):\n",
    "        epsilon = np.random.normal(0, 1, d)\n",
    "        beta = beta_schedule(beta0, i, c=c)\n",
    "        def objective_function(eta):\n",
    "            return func(x_list[i-1,:]- eta*grad(x_list[i-1,:]) + np.sqrt(2*eta/beta)*epsilon)\n",
    "        # perform exact linesearch\n",
    "        result = minimize(objective_function, eta0, method = \"SLSQP\", bounds=bounds)\n",
    "        eta = result.x\n",
    "        x_list[i,:] = x_list[i-1,:] - eta*grad(x_list[i-1,:]) + np.sqrt(2*eta/beta)*epsilon\n",
    "        f_list[i] = func(x_list[i,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07804a73-d447-49f5-ac3f-cb25ba92a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, d, func, grad, maxiter = 100, eta0 = 0.001):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Descent with exact linesearch on the step size eta\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0 (float): initial value for eta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    x_list[0,:] = x0\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    f_list[0] = func(x0)\n",
    "    for i in range(1,maxiter):\n",
    "        def objective_function(eta):\n",
    "            return func(x_list[i-1,:]- eta*grad(x_list[i-1,:]))\n",
    "        # perform exact linesearch\n",
    "        result = minimize(objective_function, eta0, method = \"SLSQP\", bounds=bounds)\n",
    "        eta = result.x\n",
    "        x_list[i,:] = x_list[i-1,:] - eta*grad(x_list[i-1,:])\n",
    "        f_list[i] = func(x_list[i,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "475b3eb1-0225-4564-92e2-fe9d21345503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_agd_x_noise(x0, d, func, grad, theta, maxiter = 100, L = None, C1 = 1):\n",
    "    \"\"\"\n",
    "    This code implements Accelerated Langevin with Perturbed x\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        theta (float): momentum parameter\n",
    "        L (float): Lipschitz constant\n",
    "        C1 (float): constant for scaling beta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    y_list = np.zeros((maxiter,d))\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    x_list[0,:] = x0\n",
    "    f_list[0] = func(x_list[0,:])\n",
    "    for i in range(maxiter-1):\n",
    "        eps = np.random.normal(0, 1, size = (d,))\n",
    "        eta = 1/L\n",
    "        beta = C1*(i+1)**(2/3)\n",
    "        xi = x_list[i,:] + np.sqrt(2*eta/beta)*eps\n",
    "        y_list[i+1,:] = xi - eta*grad(xi)\n",
    "        x_list[i+1,:] = y_list[i+1,:] + (1-theta)*(y_list[i+1,:]-y_list[i,:])\n",
    "        f_list[i+1] = func(x_list[i+1,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d59fcac0-6819-4349-b32d-ab681eadad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_agd_y_noise(x0, d, func, grad, theta, maxiter = 100, L = None, C1 = 1):\n",
    "    \"\"\"\n",
    "    This code implements Accelerated Langevin with Perturbed y\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        theta (float): momentum parameter\n",
    "        L (float): Lipschitz constant\n",
    "        C1 (float): constant for scaling beta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    y_list = np.zeros((maxiter,d))\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    x_list[0,:] = x0\n",
    "    f_list[0] = func(x_list[0,:])\n",
    "    for i in range(maxiter-1):\n",
    "        eps = np.random.normal(0, 1, size = (d,))\n",
    "        eta = 1/L\n",
    "        beta = C1*(i+1)**(2/3)\n",
    "        y_list[i+1,:] = x_list[i,:] - eta*grad(x_list[i,:]) + np.sqrt(2*eta/beta)*eps\n",
    "        x_list[i+1,:] = y_list[i+1,:] + (1-theta)*(y_list[i+1,:]-y_list[i,:])\n",
    "        f_list[i+1] = func(x_list[i+1,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a4d4c33-31be-43c5-81e3-27c2c7b52350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(x0, d, func, grad, theta, maxiter = 100, eta = 0.01):\n",
    "    \"\"\"\n",
    "    This code implements Nesterov's Accelerated Gradient\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        theta (float): momentum parameter\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta (float): step size\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    y_list = np.zeros((maxiter,d))\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    x_list[0,:] = x0\n",
    "    f_list[0] = func(x_list[0,:])\n",
    "    for i in range(maxiter-1):\n",
    "        # theta = i/(i+3)\n",
    "        y_list[i+1,:] = x_list[i,:] - eta*grad(x_list[i,:])\n",
    "        x_list[i+1,:] = y_list[i+1,:] + (1-theta)*(y_list[i+1,:]-y_list[i,:])\n",
    "        f_list[i+1] = func(x_list[i+1,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f643fa15-5482-4f68-817f-50ba39433d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_orthogonal_matrix(n):\n",
    "    # Generate a random matrix\n",
    "    A = np.random.randn(n, n)\n",
    "    \n",
    "    # QR decomposition\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d51b75a-528c-4f6a-9bae-449d2aa62186",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = random_orthogonal_matrix(2)\n",
    "D = np.diag([1,1])\n",
    "H = Q @ D @ Q.T\n",
    "g = np.random.randn(2,1)\n",
    "def quadratic_func_well_cond(x):\n",
    "    x = x.reshape((2,1))\n",
    "    return 0.5*(x.T@H@x) + g.T@x\n",
    "\n",
    "def quadratic_grad_well_cond(x):\n",
    "    x = x.reshape((2,1))\n",
    "    gf = H@x + g\n",
    "    gf = gf.reshape((2,))\n",
    "    return gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "379ca1c6-986a-4fef-ba72-2e209e61bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = random_orthogonal_matrix(2)\n",
    "D = np.diag([1e4,1])\n",
    "H = Q @ D @ Q.T\n",
    "g = np.random.randn(2,1)\n",
    "def quadratic_func_ill_cond(x):\n",
    "    return 0.5*(x.T@H@x) + g.T@x\n",
    "\n",
    "def quadratic_grad_ill_cond(x):\n",
    "    x = x.reshape((2,1))\n",
    "    gf = H@x + g\n",
    "    gf = gf.reshape((2,))\n",
    "    return gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e8d16-d78b-4f4b-bb86-60191fff4d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
