{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc91645-1f19-4c1b-bf07-e4fedab38848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ede8e06-3500-4e95-9454-132a19d15161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_beta(beta0, i, c=1):\n",
    "    beta = np.log(np.exp(c*beta0)+i)/c\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5d0714-2739-43ae-8a48-000a992de3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = Bounds([0], [np.inf])\n",
    "def langevin(x0, d, func, grad, maxiter = 100, eta0 = 0.001, beta0 = 1, beta_schedule = log_beta, c=1):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Langevin Algorithm with exact linesearch on the step size eta\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0 (float): initial value for eta\n",
    "        beta0 (float): initial value for beta\n",
    "        beta_schedule (Callable): annealing schedule for temperature, starting with beta0 and ending with beta1\n",
    "        c (float): constant in logarithmic annealing schedule\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    x_list[0,:] = x0\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    f_list[0] = func(x0)\n",
    "    for i in range(1,maxiter):\n",
    "        epsilon = np.random.normal(0, 1, d)\n",
    "        beta = beta_schedule(beta0, i, c=c)\n",
    "        def objective_function(eta):\n",
    "            return func(x_list[i-1,:]- eta*grad(x_list[i-1,:]) + np.sqrt(2*eta/beta)*epsilon)\n",
    "        # perform exact linesearch\n",
    "        result = minimize(objective_function, eta0, method = \"SLSQP\", bounds=bounds)\n",
    "        eta = result.x\n",
    "        x_list[i,:] = x_list[i-1,:] - eta*grad(x_list[i-1,:]) + np.sqrt(2*eta/beta)*epsilon\n",
    "        f_list[i] = func(x_list[i,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fed341-d852-4971-ad36-18e886f5d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, d, func, grad, maxiter = 100, eta0 = 0.001):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Descent with exact linesearch on the step size eta\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy darray): initial point\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0 (float): initial value for eta\n",
    "\n",
    "    Output:\n",
    "        f_list (numpy darray): the list of function values for each iteration\n",
    "        x_list (numpy darray): the list of x values for each iteration\n",
    "    \"\"\"\n",
    "    x_list = np.zeros((maxiter,d))\n",
    "    x_list[0,:] = x0\n",
    "    f_list = np.zeros((maxiter,))\n",
    "    f_list[0] = func(x0)\n",
    "    for i in range(1,maxiter):\n",
    "        def objective_function(eta):\n",
    "            return func(x_list[i-1,:]- eta*grad(x_list[i-1,:]))\n",
    "        # perform exact linesearch\n",
    "        result = minimize(objective_function, eta0, method = \"SLSQP\", bounds=bounds)\n",
    "        eta = result.x\n",
    "        x_list[i,:] = x_list[i-1,:] - eta*grad(x_list[i-1,:])\n",
    "        f_list[i] = func(x_list[i,:])\n",
    "    return f_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4787f3-c34f-4154-8861-4b236268da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_compare(d, func, grad, maxiter=400, eta0_gld=1e-2, eta0_gd=1e-2, beta0=1, beta_schedule=log_beta, c=1, var=1):\n",
    "    \"\"\"\n",
    "    This code implements Gradient Langevin and Gradient Descent and:\n",
    "    1) plots the semi-log plot of average convergence curve;\n",
    "    2) plots the boxplot of the final value of both algorithms;\n",
    "    3) computes the number of iterations that achieves value below threshold;\n",
    "    4) computes Q1, median and Q3 of the final value of both algorithms;\n",
    "    5) plots the initial distribution of the sampled points and the final value of both algorithms. \n",
    "\n",
    "    Args:\n",
    "        d (int): dimension of the objective function\n",
    "        func (Callable): objective function\n",
    "        grad (Callable): gradient of objective function\n",
    "        maxiter (int): maximum number of iterations\n",
    "        eta0_gld (float): initial value for eta used in Gradient Langevin\n",
    "        eta0_gd (float): initial value for eta used in Gradient Descent\n",
    "        beta0 (float): initial value for beta\n",
    "        beta_schedule (Callable): annealing schedule for temperature, starting with beta0 and ending with beta1\n",
    "        c (float): constant in logarithmic annealing schedule\n",
    "        var (float): variance of the distribution where the initial point is sampled from\n",
    "    \n",
    "    \"\"\"\n",
    "    gld_result = None\n",
    "    gd_result = None\n",
    "    x = np.zeros((100,2))\n",
    "    gld_x = np.zeros((100,2))\n",
    "    gd_x = np.zeros((100,2))\n",
    "    for i in tqdm(range(100)):\n",
    "        x0 = np.random.normal(0,var, size=(2,))\n",
    "        x[i,:] = x0\n",
    "        f_list_gld, x_list_gld = langevin(x0, d, func, grad, maxiter=maxiter, eta0 = eta0_gld, beta0 = beta0, beta1=beta1, beta_schedule = beta_schedule, c=c)\n",
    "        f_list_grad, x_list_gd = gradient_descent(x0, d, func, grad, maxiter=maxiter, eta0 = eta0_gd)\n",
    "        gld_x[i,:] = x_list_gld[-1,:]\n",
    "        gd_x[i,:] = x_list_gd[-1,:]\n",
    "        f_list_gld = np.array(f_list_gld)\n",
    "        f_list_grad = np.array(f_list_grad)\n",
    "        f_list_gld = f_list_gld.reshape((maxiter,1))\n",
    "        f_list_grad = f_list_grad.reshape((maxiter,1))\n",
    "        if i == 0:\n",
    "            gld_result = f_list_gld\n",
    "            gd_result = f_list_grad\n",
    "        else:\n",
    "            gld_result = np.concatenate((gld_result, f_list_gld), axis = 1)\n",
    "            gd_result = np.concatenate((gd_result, f_list_grad), axis = 1)\n",
    "\n",
    "    # Extract final output\n",
    "    gld_final = gld_result[-1,:]\n",
    "    gd_final = gd_result[-1,:]\n",
    "\n",
    "    mean_gld = np.mean(gld_result, axis=1)\n",
    "    std_dev_gld = np.std(gld_result, axis=1)\n",
    "    mean_gd = np.mean(gd_result, axis=1)\n",
    "    std_dev_gd = np.std(gd_result, axis=1)\n",
    "\n",
    "    plt.plot(np.arange(0,maxiter), np.log10(mean_gld), label = \"Gradient Langevin\")\n",
    "    plt.plot(np.arange(0,maxiter), np.log10(mean_gd), label = \"Gradient Descent\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Gradient Evaluations\")\n",
    "    plt.ylabel(\"log(f(x_k) - f^*)\")\n",
    "    plt.savefig(\"mean.jpg\")\n",
    "\n",
    "    plt.figure(figsize=(4,6))\n",
    "    sns.boxplot(data=[gld_final, gd_final], palette='pastel')\n",
    "    plt.title(\"Boxplot of Final Objective Value\")\n",
    "    plt.ylabel(\"Function Value\")\n",
    "    plt.xticks([0,1], [\"Gradient Langevin\", 'Gradient Descent'])  # if you just have one list\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.savefig(\"boxplot.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Average for GLD:\", mean_gld[-1])\n",
    "    print(\"Average for GD:\", mean_gd[-1])\n",
    "\n",
    "    threshold = 1e-6\n",
    "    gld_count = sum(1 for x in gld_final if x < threshold)\n",
    "    gd_count = sum(1 for x in gd_final if x < threshold)\n",
    "    print(\"number of iterations that achieve below 1e-6 for Gradient Langevin:\", gld_count)\n",
    "    print(\"number of iterations that achieve below 1e-6 for Gradient Descent:\", gd_count)\n",
    "\n",
    "    threshold = 1e-7\n",
    "    gld_count = sum(1 for x in gld_final if x < threshold)\n",
    "    gd_count = sum(1 for x in gd_final if x < threshold)\n",
    "    print(\"number of iterations that achieve below 1e-7 for Gradient Langevin:\", gld_count)\n",
    "    print(\"number of iterations that achieve below 1e-7 for Gradient Descent:\", gd_count)\n",
    "\n",
    "    gld_med = statistics.median(gld_final)\n",
    "    gld_mean = statistics.mean(gld_final)\n",
    "    # quartiles – split into 4 bins\n",
    "    q1, q2, q3 = statistics.quantiles(gld_final, n=4)  # q2 == median\n",
    "    print(\"Q1, Q2(median), Q3, mean for Gradient Langevin:\", q1, q2, q3, gld_mean)\n",
    "    \n",
    "    gd_med = statistics.median(gd_final)\n",
    "    gd_mean = statistics.mean(gd_final)\n",
    "    # quartiles – split into 4 bins\n",
    "    q1, q2, q3 = statistics.quantiles(gd_final, n=4)  # q2 == median\n",
    "    print(\"Q1, Q2(median), Q3, mean for Gradient Descent:\", q1, q2, q3, gd_mean)\n",
    "\n",
    "    # Generate a grid over the domain\n",
    "    x_s = np.linspace(-4*var, 4*var, 400)\n",
    "    y_s = np.linspace(-4*var, 4*var, 400)\n",
    "    X, Y = np.meshgrid(x_s, y_s)\n",
    "    XY = np.stack([X.ravel(), Y.ravel()], axis=1) \n",
    "    Z = np.array([func(xy) for xy in XY])\n",
    "    Z = Z.reshape(X.shape)\n",
    "    \n",
    "    # Plot the contour and overlay the points\n",
    "    contours = plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "    plt.colorbar(contours, label='Function Value')\n",
    "    plt.scatter(x[:, 0], x[:, 1], c='white', s=20, edgecolor='k', label='Sampled Initial Points')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"initial_points.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "    contours = plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "    plt.colorbar(contours, label='Function Value')\n",
    "    plt.scatter(gld_x[:,0], gld_x[:,1], c='white', s=20, edgecolor='k', label='Gradient Langevin Algorithm Output')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"gld_conv.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    contours = plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "    plt.colorbar(contours, label='Function Value')\n",
    "    plt.scatter(gd_x[:,0], gd_x[:,1], c='white', s=20, edgecolor='k', label='Gradient Descent Algorithm Output')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"gd_conv.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d6384-9ef4-428b-885b-811eb0ddab9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
